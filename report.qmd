---
title: "Predicting Success in March Madness"
subtitle: "Report"
format: html
editor: visual
execute:
  echo: true
---

# **Introduction and Data**

Come March, each college basketball fan is focused on one goal: filling out the perfect March Madness bracket. They are trying to figure out which lower seeded teams are poised to make a surprise run and which higher seeded teams are not as good as advertised. Some have gotten close but still no bracket has been able to correctly predict all 63 games in a tournament. Even in the 2023 tournament, no websites reported a perfect bracket even after just the first day's slate of games. We have all tried as well and have failed all the same.

The goal of our project is to answer the question: What factors are the strongest predictors of success in March Madness? Our hypothesis is that teams who are able to limit their turnover percentage while remaining high in KENPOM adjusted efficiency and BARTORVIK adjusted efficiency will be able to find success in the tournament. There are many other factors at play in a game of basketball, but these are the ones that we believe will be the most indicative of success.

The source of the data is from kaggle.com. However, because a lot of the datasets on kaggle.com are fake, we made sure to cross-check the data with official NCAA data on NCAA.com. We found that the dataset contains accurate information. The data from the dataset is pulled from <https://kenpom.com/> and <https://www.barttorvik.com/#>. The data encompasses March Madness tournaments spanning from 2008 - 2023 (2020 not included). However, we will not be using the data for the 2023 March Madness tournament, as it is ongoing. Each observation in the dataset represents a team in that year's tournament.

**Relevant variables**

Turnover percentage: The percentage of a team's possessions that end in a turnover before that team is able to attempt a shot on their own basket. A turnover is defined as a team losing possession to the other team during the course of play or due to an infraction of the rules. (Ken Pomeroy)

KENPOM adjusted efficiency: Estimates how many points a team would outscore the average Division I basketball team by over the course of 100 possessions. (Ken Pomeroy)

BARTTORVIK adjusted efficiency: Estimates how many points a team would outscore the average Division I basketball team by over the course of 100 possessions. (Bart Torvik)

# **Methodology**

**Data Wrangling**

First, let's load the necessary packages:

```{r}

# | label: load-packages

library(tidyverse)
library(tidymodels)

```

Next, we will clean and read the data. First, we edited the CSV file directly to rename some variables so that RStudio could easily recognize them (i.e., removing spaces). Second, we manually added the March Madness 2023 results (since previously there were no 2023 results) so that we could also use those values within our analysis:

```{r}

# | label: read-data

march_madness <- read_csv("data/Tournament_Data.csv")

#glimpse(march_madness)
```

We also will filter out rounds that equal 0:

```{r}

# | label: clean-data

march_madness <- march_madness |>
  filter(round != 0 & round != 68) |>
  mutate(
    elite_eight = if_else(round <= 8, "elite eight", "not elite eight"),
    elite_eight = as.factor(elite_eight)
  )
```

Lastly, to set up our data, we will split it for training and testing:

```{r}

# | label: train-test

set.seed(101) 
march_madness_split <- initial_split(march_madness, prop = 0.80) 
train_data <- training(march_madness_split)
test_data <- testing(march_madness_split) 

```

**Exploratory Plots and Logistic Regression**

ModelF (Feng):

```{r}

# | label: model-f-boxplot

```

```{r}

# | label: model-f-log-reg

```

Add equation...

```{r}

# | label: model-f-AIC

```

Add narrative...

ModelM (Mansoor):

```{r}

# | label: model-m-boxplot

march_madness |> 
  mutate(round = as.factor(round)) |> 
  ggplot(aes(x = round, y = b_adj_eff)) +
  geom_boxplot()

```

```{r}

# | label: model-m-log-reg

modelm <- logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ b_adj_eff, data = train_data, family = "binomial")

tidy(modelm)
```

$\widehat{round}$ = 6.09 - 0.208 \* b_adj_eff

```{r}

# | label: model-m-AIC

cat("modelm AIC:", glance(modelm)$AIC)

```

```{r}

# | label: model-m-ROC

modelm_pred <- predict(modelm, test_data, type = "prob")|>  
  bind_cols(test_data |> select(elite_eight))

modelm_pred |>
  roc_curve(
    truth = elite_eight,
    `.pred_elite eight`,
    event_level = "first"
  ) |>
  autoplot() +
  labs(title = "ROC curve for model m")

```

```{r}

modelm_pred |>
  roc_auc(
    truth = elite_eight,
    `.pred_elite eight`, 
    event_level = "first" 
  ) 

```

Add narrative...

ModelW (Wu):

```{r}
# | label: model-w-boxplot

march_madness |> 
  mutate(round = as.factor(round)) |> 
  ggplot(aes(x = round, y = wins_above_bubble)) +
  geom_boxplot()
```

```{r}
# | label: model-w-log-reg

modelw <- logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ wins_above_bubble, data = train_data, family = "binomial")

tidy(modelw)
```

$\widehat{round} = 3.46 - 0.374 * wins\_above\_bubble$

```{r}
# | label: model-w-AIC

cat("modelw AIC:", glance(modelw)$AIC)
```

```{r}
#| label: model-w-ROC

modelw_pred <- predict(modelw, test_data, type = "prob")|>  
  bind_cols(test_data |> select(elite_eight))

modelw_pred |>
  roc_curve(
    truth = elite_eight,
    `.pred_elite eight`,
    event_level = "first"
  ) |>
  autoplot() +
  labs(title = "ROC curve for model w")
```

```{r}
#| label: model-w-AUC

modelw_pred |>
  roc_auc(
    truth = elite_eight,
    `.pred_elite eight`, 
    event_level = "first" 
  ) 
```

Add narrative...

# **Results**
