---
title: "Predicting Success in March Madness"
subtitle: "Report"
format: html
editor: visual
execute:
  echo: true
---

# **Introduction and Data**

Come March, each college basketball fan is focused on one goal: filling out the perfect March Madness bracket. They are trying to figure out which lower seeded teams are poised to make a surprise run and which higher seeded teams are not as good as advertised. Some have gotten close but still no bracket has been able to correctly predict all 63 games in a tournament. Even in the 2023 tournament, no websites reported a perfect bracket even after just the first day's slate of games. We have all tried as well and have failed all the same.

The goal of our project is to answer the question: What factors are the strongest predictors of success in March Madness? Our hypothesis is that teams who are able to limit their turnover percentage while remaining high in KENPOM adjusted efficiency and BARTORVIK adjusted efficiency will be able to find success in the tournament. There are many other factors at play in a game of basketball, but these are the ones that we believe will be the most indicative of success.

The source of the data is from kaggle.com. However, because a lot of the datasets on kaggle.com are fake, we made sure to cross-check the data with official NCAA data on NCAA.com. We found that the dataset contains accurate information. The data from the dataset is pulled from <https://kenpom.com/> and <https://www.barttorvik.com/#>. The data encompasses March Madness tournaments spanning from 2008 - 2023 (2020 not included). However, we will not be using the data for the 2023 March Madness tournament, as it is ongoing. Each observation in the dataset represents a team in that year's tournament.

**Relevant variables**

Turnover percentage: The percentage of a team's possessions that end in a turnover before that team is able to attempt a shot on their own basket. A turnover is defined as a team losing possession to the other team during the course of play or due to an infraction of the rules. (Ken Pomeroy)

KENPOM adjusted efficiency: Estimates how many points a team would outscore the average Division I basketball team by over the course of 100 possessions. (Ken Pomeroy)

BARTTORVIK adjusted efficiency: Estimates how many points a team would outscore the average Division I basketball team by over the course of 100 possessions. (Bart Torvik)

# **Methodology**

**Data Wrangling**

First, let's load the necessary packages:

```{r}

# | label: load-packages

library(tidyverse)
library(tidymodels)

```

Next, we will clean and read the data. First, we edited the CSV file directly to rename some variables so that RStudio could easily recognize them (i.e., removing spaces). Second, we manually added the March Madness 2023 results (since previously there were no 2023 results) so that we could also use those values within our analysis:

```{r}

# | label: read-data

march_madness <- read_csv("data/Tournament_Data.csv")

#glimpse(march_madness)
```

We also will filter out rounds that equal 0:

```{r}

# | label: clean-data

march_madness <- march_madness |>
  filter(round != 0 & round != 68) |>
  mutate(
    elite_eight = if_else(round <= 8, "elite eight", "not elite eight"),
    elite_eight = as.factor(elite_eight)
  )
```

Lastly, to set up our data, we will split it for training and testing:

```{r}

# | label: train-test

set.seed(101) 
march_madness_split <- initial_split(march_madness, prop = 0.80) 
train_data <- training(march_madness_split)
test_data <- testing(march_madness_split) 

```

**Exploratory Plots and Logistic Regression**

ModelF (Feng):

```{r}

# | label: model-f-boxplot

march_madness |> 
  mutate(round = as.factor(round)) |>
  ggplot(aes(x = round, y = BARTHAG)) +
  geom_boxplot()

```

BARTHAG was the best boxplot that I was able to create from the variables that I analyzed. The median score of BARTHAG increases for the most part for each round that the tournaments advances. The lower quartile especially narrows down and becomes more specific. There are quite a few outliers in the data but that is to be expected given the unpredictability and volatility of events during the NCAA tournament.

```{r}

# | label: model-f-log-reg

set.seed(101)
march_madness_split <- initial_split(march_madness, prop = 0.80) 
train_data <- training(march_madness_split)
test_data <- testing(march_madness_split) 

modelf <- logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ BARTHAG, data = train_data, family = "binomial")

logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ BARTHAG, data = train_data, family = "binomial") |>
  tidy()

cat("model1 AIC:", glance(modelf)$AIC)

```

This is the creation of a logistic regression model for using BARTHAG to predict whether or not a team will make the elite eight. The calculated AIC is 447.3113. The equation that models this logistic regression is: $\widehat{round} = 20.92 - 21.32*BARTHAG$.

```{r}

# | label: model-f-AIC

modelf_pred <- predict(model1, test_data, type = "prob")|>  
  bind_cols(test_data |> select(elite_eight))

modelf_pred |>
  roc_curve(
    truth = elite_eight,
    `.pred_elite eight`,
    event_level = "first"
  ) |>
  autoplot() +
  labs(title = "ROC curve for model f")
```
In this, we generated an ROC curve for BARTHAG to see how good the model was in predicting true positives. From the curve, it seems as if there is significant area underneath the plot.

```{r}
#| label: modelf AUC

modelf_pred |>
  roc_auc(
    truth = elite_eight,
    `.pred_elite eight`, 
    event_level = "first" 
  ) 

```
This is the calculation of the AUC for the BARTHAG model. We got a value of 89.36 for AUC, which is one of the highest scores that we got. This means that BARTHAG is a fairly good predictor of whether or not a team would make it to the elite eight round of March Madness.

ModelM (Mansoor):

```{r}

# | label: model-m-boxplot

march_madness |> 
  mutate(round = as.factor(round)) |> 
  ggplot(aes(x = round, y = b_adj_eff)) +
  geom_boxplot()

```

```{r}

# | label: model-m-log-reg

modelm <- logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ b_adj_eff, data = train_data, family = "binomial")

tidy(modelm)
```

$\widehat{round}$ = 6.09 - 0.208 \* b_adj_eff

```{r}

# | label: model-m-AIC

cat("modelm AIC:", glance(modelm)$AIC)

```

```{r}

# | label: model-m-ROC

modelm_pred <- predict(modelm, test_data, type = "prob")|>  
  bind_cols(test_data |> select(elite_eight))

modelm_pred |>
  roc_curve(
    truth = elite_eight,
    `.pred_elite eight`,
    event_level = "first"
  ) |>
  autoplot() +
  labs(title = "ROC curve for model m")

```

```{r}

modelm_pred |>
  roc_auc(
    truth = elite_eight,
    `.pred_elite eight`, 
    event_level = "first" 
  ) 

```

Add narrative...

ModelW (Wu):

```{r}
# | label: model-w-boxplot

march_madness |> 
  mutate(round = as.factor(round)) |> 
  ggplot(aes(x = round, y = wins_above_bubble)) +
  geom_boxplot()
```

```{r}
# | label: model-w-log-reg

modelw <- logistic_reg() |>
  set_engine("glm") |>
  fit(elite_eight ~ wins_above_bubble, data = train_data, family = "binomial")

tidy(modelw)
```

$\widehat{round} = 3.46 - 0.374 * wins\_above\_bubble$

```{r}
# | label: model-w-AIC

cat("modelw AIC:", glance(modelw)$AIC)
```

```{r}
#| label: model-w-ROC

modelw_pred <- predict(modelw, test_data, type = "prob")|>  
  bind_cols(test_data |> select(elite_eight))

modelw_pred |>
  roc_curve(
    truth = elite_eight,
    `.pred_elite eight`,
    event_level = "first"
  ) |>
  autoplot() +
  labs(title = "ROC curve for model w")
```

```{r}
#| label: model-w-AUC

modelw_pred |>
  roc_auc(
    truth = elite_eight,
    `.pred_elite eight`, 
    event_level = "first" 
  ) 
```

Add narrative...

# **Results**
