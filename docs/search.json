[
  {
    "objectID": "sarah.html",
    "href": "sarah.html",
    "title": "sarah",
    "section": "",
    "text": "Come March, every college basketball fan is focused on one goal: filling out the perfect March Madness bracket. They are trying to figure out which lower seeded teams are poised to make a surprise run and which higher seeded teams are not as good as advertised. Some have gotten close but still no bracket has been able to correctly predict all 63 games in a tournament. Even in the 2023 tournament, no websites reported a perfect bracket even after just the first day’s slate of games. We have all tried as well and have failed all the same.\nThe goal of our project is to answer the question: What factors are the strongest predictors of success in March Madness? We are defining “success” as a team making it into the Elite Eight of the tournament. Our hypothesis is that teams with a high BARTHAG value will most be likely to find success in the tournament.\nThe source of the data is from kaggle.com. However, because a lot of the datasets on kaggle.com are fake, we made sure to cross-check the data with official NCAA data on NCAA.com. We found that the dataset contains accurate information. The data from the dataset is pulled from https://kenpom.com/ and https://www.barttorvik.com/#. The data encompasses March Madness tournaments spanning from 2008 - 2023 (2020 not included). Since we downloaded the dataset before the conclusion of the 2023 tournament, we used data from NCAA.com to fill in the missing data for the 2023 March Madness tournament. The data wrangling is gone into more detail in the following section. Each observation in the dataset represents a team in that year’s tournament.\nData Wrangling- ADD\nRelevant variables\nRelevant variables were pulled from the dataset by first constructing many boxplots, plotting every variable in the dataset to round number. From the boxplots, we roughly estimated which ones seemed to have the highest correlation, and further analyzed them in this report. Below are the three variables we chose to investigate further. They happen to be all from https://barttorvik.com/.\nBARTHAG: The team’s chance of winning against the average DI team\nWins Above Bubble: How many more or less wins the average bubble team would have against the team’s schedule\nBarttorvik Adjusted Efficiency: Bart Torvik’s calculation of how efficient a team is offensively and defensively\n\n#| label: load-pkgs\n#| message: false\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.3     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\n\nmarch_madness <- read_csv(\"data/Tournament_Data.csv\")\n\nRows: 1011 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarch_madness <- march_madness |>\n  filter(round != 0 & round != 68) |>\n  mutate(\n    elite_eight = if_else(round <= 8, \"elite eight\", \"not elite eight\"),\n    elite_eight = as.factor(elite_eight)\n  )\n\n\nset.seed(101)\n\nmarch_madness_split <- initial_split(march_madness, prop = 0.80) \ntrain_data <- training(march_madness_split)\ntest_data <- testing(march_madness_split) \n\nmodel3 <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(elite_eight ~ wins_above_bubble, data = train_data, family = \"binomial\")\n\nglance(model3)$AIC\n\n[1] 434.0668\n\n\n\nmodel3_pred <- predict(model3, test_data, type = \"prob\")|>  \n  bind_cols(test_data |> select(elite_eight))\n\nmodel3_pred |>\n  roc_curve(\n    truth = elite_eight,\n    `.pred_elite eight`,\n    event_level = \"first\"\n  ) |>\n  autoplot() +\n  labs(title = \"ROC curve for model 3\")\n\n\n\n\n\nmodel3_pred |>\n  roc_auc(\n    truth = elite_eight,\n    `.pred_elite eight`, \n    event_level = \"first\" \n  ) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.898"
  },
  {
    "objectID": "morgan.html",
    "href": "morgan.html",
    "title": "morgan",
    "section": "",
    "text": "march_madness <- read_csv(\"data/Tournament_Data.csv\")\n\nRows: 1011 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarch_madness <- march_madness |>\n  filter(round != 0 & round != 68) |>\n  mutate(\n    ##round = as.factor(round),\n    elite_eight = if_else(round <= 8, \"elite eight\", \"not elite eight\"),\n    elite_eight = as.factor(elite_eight)\n  )\n\nmarch_madness\n\n# A tibble: 960 × 41\n    year  seed team      round k_adj_eff k_adj…¹ k_adj…² k_adj…³ b_adj…⁴ b_adj…⁵\n   <dbl> <dbl> <chr>     <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  2023     1 Alabama      16      26.9    116.    88.9    72.8    27.6    117.\n 2  2023     1 Houston      16      29.9    119.    89.0    63.4    28.3    118.\n 3  2023     1 Kansas       32      23.1    115.    91.6    69.2    21.4    114.\n 4  2023     1 Purdue       64      24.8    120.    95.2    64.2    25.2    119.\n 5  2023     2 Arizona      64      22.9    120.    97.2    72.3    23.9    120.\n 6  2023     2 Marquette    32      21.8    119.    97.6    68.3    21.9    119.\n 7  2023     2 Texas         8      24.6    117.    92.1    69.1    23.5    116.\n 8  2023     2 UCLA         16      27.7    115.    87.3    66.2    25.1    114.\n 9  2023     3 Baylor       32      20.0    122.   102.     66.5    22.3    123.\n10  2023     3 Gonzaga       8      24.3    124.    99.8    70.0    25.5    125.\n# … with 950 more rows, 31 more variables: b_adj_def <dbl>, BARTHAG <dbl>,\n#   elite_sos <dbl>, b_adj_tempo <dbl>, two_pt_perc <dbl>, three_pt_perc <dbl>,\n#   ft_perc <dbl>, efg_perc <dbl>, ft_rate <dbl>, three_pt_rate <dbl>,\n#   assist_perc <dbl>, off_reb_perc <dbl>, def_reb_perc <dbl>,\n#   block_perc <dbl>, turnover_perc <dbl>, two_pt_perc_def <dbl>,\n#   three_pt_perc_def <dbl>, ft_perc_def <dbl>, efg_perc_defense <dbl>,\n#   ft_rate_def <dbl>, three_pt_rate_def <dbl>, op_assist_perc <dbl>, …\n\n\n\nmarch_madness |> \n  ggplot(aes(x = BARTHAG, y = round)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\n\nset.seed(101)\nmarch_madness_split <- initial_split(march_madness, prop = 0.80) \ntrain_data <- training(march_madness_split)\ntest_data <- testing(march_madness_split) \n\nmodel1 <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(elite_eight ~ BARTHAG, data = train_data, family = \"binomial\")\n\ncat(\"model1 AIC:\", glance(model1)$AIC)\n\nmodel1 AIC: 447.3113\n\n\n\nset.seed(101)\nmodel1_pred <- predict(model1, test_data, type = \"prob\")|>  \n  bind_cols(test_data |> select(elite_eight))\n\nmodel1_pred |>\n  roc_curve(\n    truth = elite_eight,\n    `.pred_elite eight`,\n    event_level = \"first\"\n  ) |>\n  autoplot() +\n  labs(title = \"ROC curve for model 1\")\n\n\n\n\n\nset.seed(101)\nmodel1_pred |>\n  roc_auc(\n    truth = elite_eight,\n    `.pred_elite eight`, \n    event_level = \"first\" \n  ) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.894"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Project title",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "proposal.html#introduction-and-data",
    "href": "proposal.html#introduction-and-data",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\n\nThe dataset was found on Kaggle.com: https://www.kaggle.com/datasets/nishaanamin/march-madness-data.\n\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\n\nThe data is pulled from https://kenpom.com/ and https://www.barttorvik.com/#. The data encompasses March Madness tournaments spanning from 2008 - 2023 (2022 not included). We cross-checked with NCAA.com to ensure that the data was accurate.\n\nWrite a brief description of the observations.\n\nThere are 1011 observations and 41 variables in the dataset. They tackle questions about game & team statistics, details about each March Madness tournament, and more."
  },
  {
    "objectID": "proposal.html#research-question",
    "href": "proposal.html#research-question",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\n\nWhat statistics, tournament information, and overall predictors are the best for predicting a potential upset?\n\nWe are no longer looking into upsets.\n\nDeveloping question (not fleshed out): Does the 2023 March Madness winner align with our model prediction? (Does our model’s predicted “most likely to be successful” teams align with the winner of March Madness?)\nNEW: What factors are the strongest predictors of success in March Madness?\n\nThese questions are important because March Madness is a pivotal and important part of the NCAA, and learning more about how to analyze the tournament through data science can foster the intersection between sports and technology. By answering them, we can learn more about what factors are important when predicting a winner. The questions themselves are not particularly original, but our insights can be valuable and unique. The target population is anyone interested in March Madness, the NCAA or NBA, or general basketball/sports enthusiasts.\n\n\nA description of the research topic along with a concise statement of your hypotheses on this topic.\n\nFocusing on the first research question, our research topic is using the Tournament Team Data from 2008-2022 to try and figure out which predictor will be the best to predict success. Our hypothesis is that teams that score the most and give up the most points will do the best. In addition, we will look at previous upsets and analyze trends. Overall, however, we would need to play around with the dataset more before settling on a finalized hypothesis.\n\nIdentify the types of variables in your research question. Categorical? Quantitative?\n\nThere are both categorical variables (e.g., Seed, Year, and Round) and quantitative variables (e.g., Points per Possession Offensive, Points per Possession Defensive, and Strength of Schedule). There are many variables in our dataset. Within our project, we will be testing the variables in our dataset to discover which variables are the strongest predictors."
  },
  {
    "objectID": "proposal.html#literature",
    "href": "proposal.html#literature",
    "title": "Project title",
    "section": "Literature",
    "text": "Literature\n\nFind one published credible article on the topic you are interested in researching.\n\nDowns, Sarah. “Using Statistics to Create the Perfect March Madness Bracket.” Journal of Sports Analytics, vol. 5, no. 2, 2019. https://escholarship.org/uc/item/7s99n4nq\n\nProvide a one paragraph summary about the article.\n\nThe use of statistical analysis to forecast the results of the yearly NCAA Men’s Basketball Tournament, often known as March Madness, is explored in the article “Using Statistics to Build the Ideal March Madness Bracket” by Sarah Downs. The article explores different statistical forecasting techniques and emphasizes the significance of elements like team performance measures, player statistics, and historical trends. The author also highlights the possible advantages of employing statistical models to choose brackets in a more informed manner, such as raising the likelihood of succeeding in office pools or online competitions.\n\nIn 1-2 sentences, explain how your research question builds on / is different than the article you have cited.\n\nWhile the article discusses different metrics for statistical forecasting, we are 1). specific to predicting success, and 2). we use different variables."
  },
  {
    "objectID": "proposal.html#glimpse-of-data",
    "href": "proposal.html#glimpse-of-data",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nmarch_madness <- read_csv(\"data/Tournament_Data.csv\")\n\nRows: 1011 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(march_madness)\n\nRows: 1,011\nColumns: 40\n$ year                  <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, …\n$ seed                  <dbl> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, …\n$ team                  <chr> \"Alabama\", \"Houston\", \"Kansas\", \"Purdue\", \"Arizo…\n$ round                 <dbl> 16, 16, 32, 64, 64, 32, 8, 16, 32, 8, 8, 16, 1, …\n$ k_adj_eff             <dbl> 26.9363, 29.8743, 23.1345, 24.7732, 22.8757, 21.…\n$ k_adj_off             <dbl> 115.832, 118.854, 114.703, 119.951, 120.078, 119…\n$ k_adj_def             <dbl> 88.8960, 88.9798, 91.5684, 95.1774, 97.2023, 97.…\n$ k_adj_tempo           <dbl> 72.7700, 63.3827, 69.2256, 64.2174, 72.2672, 68.…\n$ b_adj_eff             <dbl> 27.588, 28.290, 21.359, 25.247, 23.914, 21.894, …\n$ b_adj_off             <dbl> 116.981, 118.260, 113.750, 119.495, 120.450, 119…\n$ b_adj_def             <dbl> 89.393, 89.970, 92.391, 94.248, 96.536, 97.159, …\n$ BARTHAG               <dbl> 0.957, 0.959, 0.916, 0.939, 0.927, 0.912, 0.931,…\n$ elite_sos             <dbl> 33.867, 23.551, 38.469, 29.575, 28.591, 28.741, …\n$ b_adj_tempo           <dbl> 72.704, 63.262, 69.116, 64.077, 72.427, 68.627, …\n$ two_pt_perc           <dbl> 54.6, 53.2, 52.9, 54.1, 56.6, 58.8, 53.8, 50.4, …\n$ three_pt_perc         <dbl> 33.8, 34.5, 34.4, 32.6, 38.2, 34.8, 33.9, 34.7, …\n$ ft_perc               <dbl> 72.6, 71.9, 71.7, 74.3, 70.9, 72.1, 75.2, 72.2, …\n$ efg_perc              <dbl> 52.7, 52.7, 52.4, 52.2, 56.8, 56.0, 52.7, 50.9, …\n$ ft_rate               <dbl> 36.7, 28.5, 29.8, 37.2, 36.5, 27.1, 31.5, 27.1, …\n$ three_pt_rate         <dbl> 47.8, 37.8, 34.1, 38.1, 38.0, 41.9, 35.0, 29.1, …\n$ assist_perc           <dbl> 54.8, 53.8, 59.5, 60.9, 65.3, 59.3, 57.1, 50.9, …\n$ off_reb_perc          <dbl> 33.8, 37.1, 28.4, 38.3, 31.1, 26.5, 28.3, 33.7, …\n$ def_reb_perc          <dbl> 72.4, 72.6, 71.7, 76.7, 73.8, 67.9, 70.6, 72.4, …\n$ block_perc            <dbl> 11.3, 16.4, 10.0, 9.7, 8.3, 8.7, 10.1, 11.9, 6.8…\n$ turnover_perc         <dbl> 19.0, 15.3, 17.5, 17.0, 18.4, 15.2, 16.5, 15.3, …\n$ two_pt_perc_def       <dbl> 41.2, 43.1, 47.2, 47.4, 45.9, 50.0, 47.2, 46.8, …\n$ three_pt_perc_def     <dbl> 28.1, 27.8, 31.2, 31.4, 32.8, 35.2, 32.7, 31.1, …\n$ ft_perc_def           <dbl> 71.4, 69.0, 72.3, 73.6, 71.4, 69.8, 72.4, 69.7, …\n$ efg_perc_defense      <dbl> 41.5, 42.5, 47.1, 47.2, 47.2, 51.1, 47.8, 46.8, …\n$ ft_rate_def           <dbl> 32.8, 35.8, 31.2, 19.3, 26.0, 29.0, 34.5, 27.6, …\n$ three_pt_rate_def     <dbl> 30.6, 43.7, 35.1, 35.4, 38.7, 37.5, 35.4, 39.3, …\n$ op_assist_perc        <dbl> 41.8, 53.6, 46.2, 51.5, 49.4, 58.6, 48.3, 51.3, …\n$ op_off_reb_perc       <dbl> 27.6, 27.4, 28.3, 23.3, 26.2, 32.1, 29.4, 27.6, …\n$ op_def_reb_perc       <dbl> 66.2, 62.9, 71.6, 61.7, 68.9, 73.5, 71.7, 66.3, …\n$ blocked_perc          <dbl> 11.4, 6.6, 7.3, 8.6, 9.3, 9.6, 8.3, 6.3, 9.3, 5.…\n$ turnover_perc_def     <dbl> 16.0, 22.0, 20.2, 15.4, 16.9, 22.8, 22.8, 23.6, …\n$ wins_above_bubble     <dbl> 10.6, 8.4, 10.5, 9.2, 7.5, 7.5, 8.0, 8.4, 5.7, 7…\n$ win_perc              <dbl> 85.29412, 91.17647, 79.41176, 85.29412, 82.35294…\n$ pointers_per_poss_off <dbl> 1.106, 1.158, 1.062, 1.135, 1.134, 1.132, 1.095,…\n$ points_per_poss_def   <dbl> 0.922, 0.872, 0.963, 0.974, 0.981, 1.000, 0.948,…"
  },
  {
    "objectID": "proposal.html#introduction-and-data-1",
    "href": "proposal.html#introduction-and-data-1",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\nThe data was found on Kaggle (https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset).\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nThe dataset was found by compiling datasets found online from medical centers by its creator Rashik Rahman. It is not exactly clear when the dataset was created, but it has been updated yearly since it was created.\nWrite a brief description of the observations.\nThe observations are centered around medical testing and data for patients. There are things like age, heart rate, and other useful data to know when trying to diagnose someone with a heart attack. In total, there are 353 observations and 14 variables."
  },
  {
    "objectID": "proposal.html#research-question-1",
    "href": "proposal.html#research-question-1",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.\n\nWhat measurements and patient information is the best predictor for whether or not the patient will have a heart attack?\n\nThis is a significant question because heart attacks are one of the leading causes of death in America so being better able to recognize signs of one would help human health tremendously.\n\n\nA description of the research topic along with a concise statement of your hypotheses on this topic.\n\nHeart attacks are one of the leading causes of death in America. One of the reasons why they are so fatal is because they are hard to track before they actually happen. This is something that is important for health officials to get better at in order to help improve patient health. Being better able to predict likelihood of a heart attack from data already available would be a highly useful tool. Our hypothesis on the topic is that people who are older, people with a higher resting heart rate, and people who experience chest pain are more likely to experience a heart attack.\n\nIdentify the types of variables in your research question. Categorical? Quantitative?\n\nChest pain and heart attack are all categorical variables. Resting heart rate and age are quantitative variables."
  },
  {
    "objectID": "proposal.html#literature-1",
    "href": "proposal.html#literature-1",
    "title": "Project title",
    "section": "Literature",
    "text": "Literature\n\nFind one published credible article on the topic you are interested in researching.\n\nAlexander CA, Wang L (2017) Big Data Analytics in Heart Attack Prediction. J Nurs Care 6: 393. doi:10.4172/2167-1168.1000393\n\nProvide a one paragraph summary about the article.\n\nThis article focuses on using heart attack data to make a prediction about finding heart attacks before they happen. It takes data from thousands of heart attack patients to try to develop a model to better predict heart attacks and prevent them. In particular, they also focus on how getting certain measurements or readings for a test should impact the care that a patient is receiving from their care provider. Much of the article explains how they sourced their data and how they eventually were able to create a model that they felt was useful in big data and heart attacks.\n\nIn 1-2 sentences, explain how your research question builds on / is different than the article you have cited.\n\nOur research question builds on the article by continuing to add more variables to the Big Data Equation and finding an even more accurate and concise model for predicting heart attacks."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-1",
    "href": "proposal.html#glimpse-of-data-1",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nheart <- read_csv(\"data/heart.csv\")\n\nRows: 303 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trtbps, chol, fbs, restecg, thalachh, exng, oldpeak,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(heart)\n\nRows: 303\nColumns: 14\n$ age      <dbl> 63, 37, 41, 56, 57, 57, 56, 44, 52, 57, 54, 48, 49, 64, 58, 5…\n$ sex      <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1…\n$ cp       <dbl> 3, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 3, 3, 2, 2, 3, 0, 3, 0…\n$ trtbps   <dbl> 145, 130, 130, 120, 120, 140, 140, 120, 172, 150, 140, 130, 1…\n$ chol     <dbl> 233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, 2…\n$ fbs      <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ restecg  <dbl> 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1…\n$ thalachh <dbl> 150, 187, 172, 178, 163, 148, 153, 173, 162, 174, 160, 139, 1…\n$ exng     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ oldpeak  <dbl> 2.3, 3.5, 1.4, 0.8, 0.6, 0.4, 1.3, 0.0, 0.5, 1.6, 1.2, 0.2, 0…\n$ slp      <dbl> 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 1…\n$ caa      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0…\n$ thall    <dbl> 1, 2, 2, 2, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3…\n$ output   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…"
  },
  {
    "objectID": "proposal.html#introduction-and-data-2",
    "href": "proposal.html#introduction-and-data-2",
    "title": "Project title",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\n\nThe dataset was downloaded from here: https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention\n\n\n\n\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\n\nThe data was published in the paper, “Predicting Student Dropout and Academic Success” in October of 2022. The data is from students enrolled in the Instituto Politécnico de Portalegre from 2008/09 to 2018/19. Data was gathered from AMS, PAE, DGES, PORDATA (refer to paper for more details).\nhttps://doi.org/10.3390/data7110146\n\n\n\n\nWrite a brief description of the observations.\n\nThere are 4424 observations and 35 variables. Each observation represents one student, and the variables include both categorical and quantitative variables.\n\nThere does not seem to be any ethical concerns about the data."
  },
  {
    "objectID": "proposal.html#research-question-2",
    "href": "proposal.html#research-question-2",
    "title": "Project title",
    "section": "Research question",
    "text": "Research question\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\n\nComparing students who dropped out vs those who graduated, is there a significant difference between age at enrollment and if they had daytime or evening attendance?\n\nThis question is important because by noticing trends in dropouts, institutions can predict who are most likely to dropout according to demographics and provide them additional support, if needed.\n\n\nA description of the research topic along with a concise statement of your hypotheses on this topic.\n\nThe research topic is on college dropouts. Students drop out of school for many reasons, and it can be important to note if there are any significant trends among dropouts. This dataset provides various demographic data that we plan to compare for dropouts vs those who graduated. Our hypothesis is that dropouts will tend to be older and a greater proportion will have evening attendance compared to those who graduated from the institution.\n\nIdentify the types of variables in your research question. Categorical? Quantitative?\n\nVariable 1: Target, categorical\nVariable 2: Age, quantitative\nVariable 3: Daytime/evening attendance, categorical"
  },
  {
    "objectID": "proposal.html#literature-2",
    "href": "proposal.html#literature-2",
    "title": "Project title",
    "section": "Literature",
    "text": "Literature\n\nFind one published credible article on the topic you are interested in researching.\n\nCrosta, Peter Michael. “Characteristics of early community college dropouts.” (2013).\n\nProvide a one paragraph summary about the article.\n\nThe article aims to discover trends in community college drop outs since a student who drops out of community college is very costly for federal, state, and local authorities. The article looks at students who are part-time students as well, and it also specifically looked at students who dropped out early in their college career, rather than looking at all dropouts as a whole. These early dropouts are compared to students who were enrolled at least twice within four terms. The article found that students who dropped out tended to be older, had less financial aid, and performed worse academically compared to the students who were still enrolled. Some other variables were also measured, such as socioeconomic status index, that did not show a significant difference between the two groups.\n\nIn 1-2 sentences, explain how your research question builds on / is different than the article you have cited.\n\nOur research question builds on this article as we look at dropouts from an institution rather than a community college and we add in the variable of daytime vs evening attendance, which we hypothesize has a significant difference between dropouts and graduates. We also compare dropouts to those that graduated instead of those still enrolled, since it is possible that those compared to the dropouts in the article dropped out at a later term."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-2",
    "href": "proposal.html#glimpse-of-data-2",
    "title": "Project title",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nacademic_success <- read_csv(\"data/dataset.csv\")\n\nRows: 4424 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Target\ndbl (34): Marital status, Application mode, Application order, Course, Dayti...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(academic_success)\n\nRows: 4,424\nColumns: 35\n$ `Marital status`                                 <dbl> 1, 1, 1, 1, 2, 2, 1, …\n$ `Application mode`                               <dbl> 8, 6, 1, 8, 12, 12, 1…\n$ `Application order`                              <dbl> 5, 1, 5, 2, 1, 1, 1, …\n$ Course                                           <dbl> 2, 11, 5, 15, 3, 17, …\n$ `Daytime/evening attendance`                     <dbl> 1, 1, 1, 1, 0, 0, 1, …\n$ `Previous qualification`                         <dbl> 1, 1, 1, 1, 1, 12, 1,…\n$ Nacionality                                      <dbl> 1, 1, 1, 1, 1, 1, 1, …\n$ `Mother's qualification`                         <dbl> 13, 1, 22, 23, 22, 22…\n$ `Father's qualification`                         <dbl> 10, 3, 27, 27, 28, 27…\n$ `Mother's occupation`                            <dbl> 6, 4, 10, 6, 10, 10, …\n$ `Father's occupation`                            <dbl> 10, 4, 10, 4, 10, 8, …\n$ Displaced                                        <dbl> 1, 1, 1, 1, 0, 0, 1, …\n$ `Educational special needs`                      <dbl> 0, 0, 0, 0, 0, 0, 0, …\n$ Debtor                                           <dbl> 0, 0, 0, 0, 0, 1, 0, …\n$ `Tuition fees up to date`                        <dbl> 1, 0, 0, 1, 1, 1, 1, …\n$ Gender                                           <dbl> 1, 1, 1, 0, 0, 1, 0, …\n$ `Scholarship holder`                             <dbl> 0, 0, 0, 0, 0, 0, 1, …\n$ `Age at enrollment`                              <dbl> 20, 19, 19, 20, 45, 5…\n$ International                                    <dbl> 0, 0, 0, 0, 0, 0, 0, …\n$ `Curricular units 1st sem (credited)`            <dbl> 0, 0, 0, 0, 0, 0, 0, …\n$ `Curricular units 1st sem (enrolled)`            <dbl> 0, 6, 6, 6, 6, 5, 7, …\n$ `Curricular units 1st sem (evaluations)`         <dbl> 0, 6, 0, 8, 9, 10, 9,…\n$ `Curricular units 1st sem (approved)`            <dbl> 0, 6, 0, 6, 5, 5, 7, …\n$ `Curricular units 1st sem (grade)`               <dbl> 0.00000, 14.00000, 0.…\n$ `Curricular units 1st sem (without evaluations)` <dbl> 0, 0, 0, 0, 0, 0, 0, …\n$ `Curricular units 2nd sem (credited)`            <dbl> 0, 0, 0, 0, 0, 0, 0, …\n$ `Curricular units 2nd sem (enrolled)`            <dbl> 0, 6, 6, 6, 6, 5, 8, …\n$ `Curricular units 2nd sem (evaluations)`         <dbl> 0, 6, 0, 10, 6, 17, 8…\n$ `Curricular units 2nd sem (approved)`            <dbl> 0, 6, 0, 5, 6, 5, 8, …\n$ `Curricular units 2nd sem (grade)`               <dbl> 0.00000, 13.66667, 0.…\n$ `Curricular units 2nd sem (without evaluations)` <dbl> 0, 0, 0, 0, 0, 5, 0, …\n$ `Unemployment rate`                              <dbl> 10.8, 13.9, 10.8, 9.4…\n$ `Inflation rate`                                 <dbl> 1.4, -0.3, 1.4, -0.8,…\n$ GDP                                              <dbl> 1.74, 0.79, 1.74, -3.…\n$ Target                                           <chr> \"Dropout\", \"Graduate\"…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "heartemoji",
    "section": "",
    "text": "Every year for the March Madness tournament, tens of millions of people around the world fill out a bracket, hoping that maybe this year they will achieve the perfect bracket. Yet, a perfect bracket is extremely rare. In this project, we used March Madness tournament data from 2008-2022 (no data for 2020) to try to identify the strongest predictors of a team making it to the Elite Eight of the tournament. We focused on the variables, BARTHAG, Wins Above Bubble, and Barttorvik Adjusted Efficiency and fitted them with a logistic regression model. We also tested all the combinations of the variables using an additive logistic regression model. Although we found that most of the models performed roughly the same, the additive model with BARTHAG and Wins Above Bubble was our best model based on AUC. Using this model, we were able to correctly predict 2 out of 8 teams for the 2023 March Madness tournament."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Predicting Success in March Madness",
    "section": "",
    "text": "Methodology\nWe first loaded in the necessary packages:\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.3     \n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n\n\nRows: 1011 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 67 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 1,011\nColumns: 40\n$ year                  <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, …\n$ seed                  <dbl> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, …\n$ team                  <chr> \"Alabama\", \"Houston\", \"Kansas\", \"Purdue\", \"Arizo…\n$ round                 <dbl> 16, 16, 32, 64, 64, 32, 8, 16, 32, 8, 8, 16, 1, …\n$ k_adj_eff             <dbl> 26.9363, 29.8743, 23.1345, 24.7732, 22.8757, 21.…\n$ k_adj_off             <dbl> 115.832, 118.854, 114.703, 119.951, 120.078, 119…\n$ k_adj_def             <dbl> 88.8960, 88.9798, 91.5684, 95.1774, 97.2023, 97.…\n$ k_adj_tempo           <dbl> 72.7700, 63.3827, 69.2256, 64.2174, 72.2672, 68.…\n$ b_adj_eff             <dbl> 27.588, 28.290, 21.359, 25.247, 23.914, 21.894, …\n$ b_adj_off             <dbl> 116.981, 118.260, 113.750, 119.495, 120.450, 119…\n$ b_adj_def             <dbl> 89.393, 89.970, 92.391, 94.248, 96.536, 97.159, …\n$ BARTHAG               <dbl> 0.957, 0.959, 0.916, 0.939, 0.927, 0.912, 0.931,…\n$ elite_sos             <dbl> 33.867, 23.551, 38.469, 29.575, 28.591, 28.741, …\n$ b_adj_tempo           <dbl> 72.704, 63.262, 69.116, 64.077, 72.427, 68.627, …\n$ two_pt_perc           <dbl> 54.6, 53.2, 52.9, 54.1, 56.6, 58.8, 53.8, 50.4, …\n$ three_pt_perc         <dbl> 33.8, 34.5, 34.4, 32.6, 38.2, 34.8, 33.9, 34.7, …\n$ ft_perc               <dbl> 72.6, 71.9, 71.7, 74.3, 70.9, 72.1, 75.2, 72.2, …\n$ efg_perc              <dbl> 52.7, 52.7, 52.4, 52.2, 56.8, 56.0, 52.7, 50.9, …\n$ ft_rate               <dbl> 36.7, 28.5, 29.8, 37.2, 36.5, 27.1, 31.5, 27.1, …\n$ three_pt_rate         <dbl> 47.8, 37.8, 34.1, 38.1, 38.0, 41.9, 35.0, 29.1, …\n$ assist_perc           <dbl> 54.8, 53.8, 59.5, 60.9, 65.3, 59.3, 57.1, 50.9, …\n$ off_reb_perc          <dbl> 33.8, 37.1, 28.4, 38.3, 31.1, 26.5, 28.3, 33.7, …\n$ def_reb_perc          <dbl> 72.4, 72.6, 71.7, 76.7, 73.8, 67.9, 70.6, 72.4, …\n$ block_perc            <dbl> 11.3, 16.4, 10.0, 9.7, 8.3, 8.7, 10.1, 11.9, 6.8…\n$ turnover_perc         <dbl> 19.0, 15.3, 17.5, 17.0, 18.4, 15.2, 16.5, 15.3, …\n$ two_pt_perc_def       <dbl> 41.2, 43.1, 47.2, 47.4, 45.9, 50.0, 47.2, 46.8, …\n$ three_pt_perc_def     <dbl> 28.1, 27.8, 31.2, 31.4, 32.8, 35.2, 32.7, 31.1, …\n$ ft_perc_def           <dbl> 71.4, 69.0, 72.3, 73.6, 71.4, 69.8, 72.4, 69.7, …\n$ efg_perc_defense      <dbl> 41.5, 42.5, 47.1, 47.2, 47.2, 51.1, 47.8, 46.8, …\n$ ft_rate_def           <dbl> 32.8, 35.8, 31.2, 19.3, 26.0, 29.0, 34.5, 27.6, …\n$ three_pt_rate_def     <dbl> 30.6, 43.7, 35.1, 35.4, 38.7, 37.5, 35.4, 39.3, …\n$ op_assist_perc        <dbl> 41.8, 53.6, 46.2, 51.5, 49.4, 58.6, 48.3, 51.3, …\n$ op_off_reb_perc       <dbl> 27.6, 27.4, 28.3, 23.3, 26.2, 32.1, 29.4, 27.6, …\n$ op_def_reb_perc       <dbl> 66.2, 62.9, 71.6, 61.7, 68.9, 73.5, 71.7, 66.3, …\n$ blocked_perc          <dbl> 11.4, 6.6, 7.3, 8.6, 9.3, 9.6, 8.3, 6.3, 9.3, 5.…\n$ turnover_perc_def     <dbl> 16.0, 22.0, 20.2, 15.4, 16.9, 22.8, 22.8, 23.6, …\n$ wins_above_bubble     <dbl> 10.6, 8.4, 10.5, 9.2, 7.5, 7.5, 8.0, 8.4, 5.7, 7…\n$ win_perc              <dbl> 85.29412, 91.17647, 79.41176, 85.29412, 82.35294…\n$ pointers_per_poss_off <dbl> 1.106, 1.158, 1.062, 1.135, 1.134, 1.132, 1.095,…\n$ points_per_poss_def   <dbl> 0.922, 0.872, 0.963, 0.974, 0.981, 1.000, 0.948,…\n\n\nWe will filter out rounds that equal 0 or 68 as these represent teams that did not make it into the final 64 team tournament. Finally we made a new variable called “elite_eight” that stated whether or not a team made it to at least the elite eight. This was our determined measure of success as a team making it to the elite eight had to win 3 games in a row and would essentially end within the top 12.5% of all tournament teams.\n\n\n\nLastly, to set up our data, we will split it for training and testing. Training and testing data is important to avoid overfitting:\n\n\n\nExploratory Plots and Logistic Regression\nWe decided to use logistic regression for our project because we are trying to predict success (making it to the Elite Eight), and our response variable is a categorical variable (round).\nWe further calculate AIC, make a ROC plot, and calculate AUC under the ROC plot to help us find the best-fit model of the three variables we investigated and identify which variable is the best predictor of success in the tournament. The following are the three best models that we found from our initial exploratory data analysis.\nModel_BARTHAG:\n\n\n\n\n\nBARTHAG was the best boxplot that I was able to create from the variables that I analyzed. The median score of BARTHAG increases for the most part for each round that the tournaments advances. The lower quartile especially narrows down and becomes more specific. There are quite a few outliers in the data but that is to be expected given the unpredictability and volatility of events during the NCAA tournament.\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     20.9      2.70      7.76 8.61e-15\n2 BARTHAG        -21.3      2.94     -7.24 4.38e-13\n\n\nThis is the creation of a logistic regression model for using BARTHAG to predict whether or not a team will make the elite eight. The equation that models this logistic regression is:\n\\(log(\\hat{p} / (1 - \\hat{p})) = 20.92 - 21.32*BARTHAG\\)\n\n\n\n\n\nIn this, we generated an ROC curve for BARTHAG to see how good the model was in predicting true positives. Now, we will calculate the area under the curve (AUC).\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.894\n\n\nThis is the calculation of the AUC for the BARTHAG model. We got a value of 0.894 for AUC, which is very close to 1, which would have meant that BARTHAG perfectly predicts elite eight teams. This means that BARTHAG is a fairly good predictor of whether or not a team would make it to the elite eight round of March Madness.\nModel Barttorvik Adjusted Efficiency (BAE):\n\n\n\n\n\nThe median barttorvik adjusted efficiency generally rises as the event progresses through rounds. As the competition progresses, BAE variability likewise reduces. The data contains a significant number of outliers, but this is to be expected given the unpredictable and volatile nature of occurrences throughout the NCAA tournament. However, an overall association can be generally observed: BAE seems to increase as the competition progresses.\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.09     0.526      11.6  5.12e-31\n2 b_adj_eff     -0.208    0.0230     -9.05 1.40e-19\n\n\n\\(log(\\hat{p} / (1 - \\hat{p})) = 6.09 - 0.208 * b\\_adj\\_eff\\)\n\n\n# A tibble: 192 × 3\n   `.pred_elite eight` `.pred_not elite eight` elite_eight    \n                 <dbl>                   <dbl> <fct>          \n 1              0.293                    0.707 not elite eight\n 2              0.313                    0.687 elite eight    \n 3              0.115                    0.885 not elite eight\n 4              0.0582                   0.942 not elite eight\n 5              0.107                    0.893 not elite eight\n 6              0.0387                   0.961 not elite eight\n 7              0.0400                   0.960 not elite eight\n 8              0.0354                   0.965 not elite eight\n 9              0.0266                   0.973 not elite eight\n10              0.0275                   0.972 not elite eight\n# … with 182 more rows\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.891\n\n\nOne of our most significant scores was an AUC of 0.891, which is the third highest score that we found. This indicates that b_adj_eff is a fairly strong indicator whether or not a team makes it to the elite eight.\nModel_WAB (WAB stands for Wins Above Bubble):\n\n\n\n\n\nwins_above_bubble was also one of the best boxplots we were able to create from the variables that we analyzed. The median wins_above_bubble increases for the most part for each round that the tournaments advances with a few exceptions. As the tournament advance, the variability of wins_above_bubble also decreases. There are quite a few outliers in the data but that is to be expected given the unpredictability and volatility of events during the NCAA tournament. However, a general correlation can be roughly seen: as the tournament advances, wins_above_bubble seems to increase.\n\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          3.46     0.246      14.1  4.19e-45\n2 wins_above_bubble   -0.374    0.0399     -9.37 7.25e-21\n\n\nThis is the creation of a logistic regression model for using wins_above_bubble to predict whether or not a team will make the elite eight. The equation that models this logistic regression is, p represents the probability of whether or not round equals 1:\n\\(log(\\hat{p} / (1 - \\hat{p})) = 3.46 - 0.374 * wins\\_above\\_bubble\\)\ne ^ -0.374 = 0.69, as wins above bubble increases by 1 unit, the odds ratio is changed by a factor of 0.69\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.898\n\n\nThis is the calculation of the AUC for the wins_above_bubble model. We got a value of 0.898 for AUC, which is the highest scores that we got. This means that wins_above_bubble is a good predictor of whether or not a team would make it to the elite eight round of March Madness.\nFitting an Additive Model\nWe believe that an additive model is better than interactive because the three stats we are measuring are not related to each other so it does not make sense for them to interact. We will also use forward selection to create our model as there are only three variables we are considering so backwards modeling is not necessary.\nThe first model we will fit and test is using BARTHAG and wins above bubble as those two had the highest AUC when tested individually.\n\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic   p.value\n  <chr>                <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)         10.1      3.34        3.02 0.00256  \n2 BARTHAG             -7.93     3.94       -2.01 0.0440   \n3 wins_above_bubble   -0.263    0.0639     -4.12 0.0000377\n\n\nFitted equation: \\(log(\\hat{p} / (1 - \\hat{p})) = 10.07 - 7.93*BARTHAG-0.26*wins\\_above\\_bubble\\)\n\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.902\n\n\nThe AUC for this mdoel is 0.902 which means that it is about 1% better than both BARTHAG and wins above bubble at predicting elite eight success individually.\nNow we test BARTHAG and BAE, our first and third best individual predictors.\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    4.13     3.79       1.09  0.275    \n2 BARTHAG        2.80     5.42       0.517 0.605    \n3 b_adj_eff     -0.235    0.0576    -4.07  0.0000470\n\n\nFitted equation: \\(log(\\hat{p} / (1 - \\hat{p})) = 4.13 + 2.80*BARTHAG-0.23*b\\_adj\\_eff\\)\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.890\n\n\nThe AUC for this model is 0.89.\nNow we will do the BAE and WAB model, our second and third best predictors.\n\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          5.07     0.617       8.22 2.06e-16\n2 wins_above_bubble   -0.201    0.0688     -2.92 3.46e- 3\n3 b_adj_eff           -0.115    0.0385     -2.99 2.77e- 3\n\n\nFitted equation: \\(log(\\hat{p} / (1 - \\hat{p})) = 5.07 - 0.20*wins\\_above\\_bubble\\) \\(-0.12*b\\_adj\\_eff\\)\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.901\n\n\nThis model has an AUC of 0.901.\nFinally we will fit an additive model of all 3 variables\n\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)          2.86     3.77       0.758 0.448  \n2 BARTHAG              3.15     5.37       0.587 0.557  \n3 wins_above_bubble   -0.203    0.0692    -2.93  0.00335\n4 b_adj_eff           -0.144    0.0641    -2.25  0.0243 \n\n\nFitted model: \\(log(\\hat{p} / (1 - \\hat{p})) = 2.86 + 3.15*BARTHAG - 0.20*wins\\_above\\_bubble\\) \\(- 0.14 * b\\_adj\\_eff\\)\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.900\n\n\nWe get a AUC value of 0.900, which is actually lower than the previous model that just used BARTHAG and wins above bubble.\nUsing our best model to predict for 2023 teams\n\n\n# A tibble: 8 × 4\n  `.pred_elite eight` `.pred_not elite eight` elite_eight     team   \n                <dbl>                   <dbl> <fct>           <chr>  \n1               0.577                   0.423 not elite eight Alabama\n2               0.490                   0.510 not elite eight Kansas \n3               0.450                   0.550 not elite eight Purdue \n4               0.437                   0.563 not elite eight Houston\n5               0.412                   0.588 not elite eight UCLA   \n6               0.359                   0.641 elite eight     Texas  \n7               0.322                   0.678 not elite eight Arizona\n8               0.308                   0.692 elite eight     Gonzaga\n\n\nOur best model predicted that Alabama, Houston, Purdue, UCLA, Kansas, Gonzaga, Texas, and Arizona would be the elite eight teams from the 2023 tournament. This is a success rate of 25% as it got 2/8 teams correct being Texas and Gonzaga. Out of these teams only 5 even made it to the Sweet 16 round. Out of the 8 teams it picked, the model predicted that all four 1 seeds, three 2 seeds, and one 3 seed, showing that it had a heavy bias for teams already rated highly by the people making the bracket. This can be expected as normally these are the teams that have performed the best in the regular season and would likely be more poised to be successful on paper. Our model is far from perfect, which is to be expected given how volatile the NCAA tournament can be.\nResults\nOur research question was: What factors are the strongest predictors of success in March Madness? In our project, we defined “success” as a team making it into the Elite Eight of the tournament. Our original hypothesis was that teams with a high BARTHAG value would be most likely to find success in the tournament.\nFrom our initial plotting of each variable to the round number, we found the following three variables to seem to be a strong predictor for success in the tournament: 1) BARTHAG (modeled as modelf), 2) Barttorvik adjusted efficiency (modeled as modelm), and 3) wins above bubble (modeled as modelw).\nWe performed a logistic regression on each variable, with the categorical response variable being round.\nAIC, which stands for Akaike information criterion, which is used to compare the fit of models relative to other models. The lower the AIC value the better the model. Below are the three AIC values for the different variables:\nBARTHAG: 447.3113\nBarttorvik adjusted efficiency: 433.7179\nWins above bubble: 434.0668\nJust comparing AIC values, the BARTHAG model had the lowest value, while the Wins Above Bubble Model has just a very slightly higher AIC. The BARTHAG model has the highest AIC value of the three variables. This shows that Barttorvik adjusted efficiency and wins above bubble seem to be the better predictors.\nROC (Receiver Operating Characteristic) and AUC (Area Under the ROC Curve) are used to measure how good of a classifier a model is, plotting true and false positive rate, compared to a random classifier. The area under the ROC curve also helps determine how good of a classifier a model is, with AUC = 1.0 being the most ideal model (the higher the AUC the better the model). Below are the three AUC values for the three different variables.\nBARTHAG (modelf): 0.894\nBarttorvik adjusted efficiency (modelm): 0.891\nWins above bubble (modelw): 0.898\nJust comparing AUC values, it shows that all three variables are pretty good predictors of success. Modelw (Wins above bubble) had the highest value, while modelf (BARTHAG) has just a very slightly lower AUC. Modelm (Barttorvik adjusted efficiency) also has just a slightly lower AUC compared to modelw and modelf. This shows that Wins above bubble seem to be the best predictor, but the other variables also show to be very good predictors.\nBack to our research question of identifying the strongest predictors of success in March Madness, we found that the three variables we investigated (BARTHAG, Barttorvik adjusted efficiency, and Wins above bubble) are all great predictors. If we were to identify just one variable as the strongest predictor of success, we would point to Wins above bubble as the strongest predictor since it has the highest AUC value and the second lowest AIC value (just higher than Barttorvik adjusted efficiency by ~0.3489.\nHowever, since we found that all three variables are strong predictors, further research can be done by creating an additive logistic regression pairing up combinations of the three variables to find if an additive logistic regression can produce an even lower AIC and higher AUC value.\n\n\nCitations\nDowns, Sarah. “Using Statistics to Create the Perfect March Madness Bracket.” Journal of Sports Analytics, vol. 5, no. 2, 2019. https://escholarship.org/uc/item/7s99n4nq\nhttps://www.barttorvik.com/#\nhttps://kenpom.com/\nhttps://www.ncaa.com/march-madness-live/scores\nhttps://www.ncaa.com/news/basketball-men/bracketiq/2023-03-16/perfect-ncaa-bracket-absurd-odds-march-madness-dream"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sarah Wu: I am in the class of 2026 and intending to major in Statistics.\nMorgan Feng: I am in the class of 2026 and intending to major in Biology and minor in Statistics.\nSophie Mansoor: I am in the class of 2026 and intending to major in Computer Science and minor in Statistics."
  },
  {
    "objectID": "sophie.html",
    "href": "sophie.html",
    "title": "sophie_report.qmd",
    "section": "",
    "text": "#| label: load-data-1\n\nmarch_madness <- read_csv(\"data/Tournament_Data.csv\")\n\nRows: 1011 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): team\ndbl (39): year, seed, round, k_adj_eff, k_adj_off, k_adj_def, k_adj_tempo, b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarch_madness <- march_madness |>\n  filter(round != 0 & round != 68) |>\n  mutate(round = as.factor(round))\n\nnames(march_madness)\n\n [1] \"year\"                  \"seed\"                  \"team\"                 \n [4] \"round\"                 \"k_adj_eff\"             \"k_adj_off\"            \n [7] \"k_adj_def\"             \"k_adj_tempo\"           \"b_adj_eff\"            \n[10] \"b_adj_off\"             \"b_adj_def\"             \"BARTHAG\"              \n[13] \"elite_sos\"             \"b_adj_tempo\"           \"two_pt_perc\"          \n[16] \"three_pt_perc\"         \"ft_perc\"               \"efg_perc\"             \n[19] \"ft_rate\"               \"three_pt_rate\"         \"assist_perc\"          \n[22] \"off_reb_perc\"          \"def_reb_perc\"          \"block_perc\"           \n[25] \"turnover_perc\"         \"two_pt_perc_def\"       \"three_pt_perc_def\"    \n[28] \"ft_perc_def\"           \"efg_perc_defense\"      \"ft_rate_def\"          \n[31] \"three_pt_rate_def\"     \"op_assist_perc\"        \"op_off_reb_perc\"      \n[34] \"op_def_reb_perc\"       \"blocked_perc\"          \"turnover_perc_def\"    \n[37] \"wins_above_bubble\"     \"win_perc\"              \"pointers_per_poss_off\"\n[40] \"points_per_poss_def\"  \n\n\n\nmarch_madness |> \n  ggplot(aes(x = round, y = seed)) +\n  geom_boxplot()\n\n\n\n\n\nmarch_madness |> \n  ggplot(aes(x = round, y = b_adj_eff)) +\n  geom_boxplot()\n\n\n\n\n\nmarch_madness |> \n  ggplot(aes(x = round, y = elite_sos)) +\n  geom_boxplot()\n\n\n\n\n\nmarch_madness |> \n  ggplot(aes(x = round, y = wins_above_bubble)) +\n  geom_boxplot()\n\n\n\n\n\nset.seed(1234) \nmarch_madness_split <- initial_split(march_madness, prop = 0.80) \ntrain_data <- training(march_madness_split)\ntest_data <- testing(march_madness_split) \n\n\nmodel2 <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(round ~ b_adj_eff, data = train_data, family = \"binomial\")\n\ntidy(model2)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)   14.4      2.65        5.44 0.0000000536\n2 b_adj_eff     -0.410    0.0899     -4.57 0.00000498  \n\n\n\\(\\widehat{round}\\) = 14.39 - 0.41 * b_adj_eff\n\ncat(\"model2 AIC:\", glance(model2)$AIC)\n\nmodel2 AIC: 61.84399"
  }
]